{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time, math\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "tf.set_random_seed(777)\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='ann'\n",
    "condition= 'release'   #release , #press\n",
    "data_split_type= 'cycle_split'  #total_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for i in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,i), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,i), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,i), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,i), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')    \n",
    "        \n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, 16])\n",
    "        Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "        W1 = tf.get_variable(\"weight1\", shape=[16, 64], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        W2 = tf.get_variable(\"weight2\", shape=[64, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        W3 = tf.get_variable(\"weight3\", shape=[128, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        b1 = tf.Variable(tf.random_normal([64]), name='bias1')\n",
    "        b2 = tf.Variable(tf.random_normal([128]), name='bias2')\n",
    "        b3 = tf.Variable(tf.random_normal([1]), name='bias3')\n",
    "\n",
    "        L2 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "        L3 = tf.nn.relu(tf.matmul(L2, W2) + b2)\n",
    "        hypothesis = tf.matmul(L3, W3) + b3\n",
    "\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "        train_op  = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "        from sklearn.metrics import r2_score\n",
    "        from sklearn.metrics import explained_variance_score\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        def run_train(sess, train_x, train_y):\n",
    "            print(\"\\nStart training\")\n",
    "            sess.run(init)\n",
    "            batch_size = 10\n",
    "            for epoch in range(300):\n",
    "                total_batch = int(train_x.shape[0] / batch_size)\n",
    "                for i in range(total_batch):\n",
    "                    batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "                    batch_y = train_y[i*batch_size:(i+1)*batch_size]\n",
    "                    _, c = sess.run([train_op, cost], feed_dict={X: batch_x, Y: batch_y})\n",
    "                print(\"Epoch #%d cost=%f\" % (epoch, c))\n",
    "\n",
    "        def cross_validate(sess, split_size=5):\n",
    "            results = []\n",
    "            kf = KFold(n_splits=split_size, shuffle=True)\n",
    "            for train_idx, val_idx in kf.split(x_data_training, y_data_training):\n",
    "                train_x = x_data_training[train_idx]\n",
    "                train_y = y_data_training[train_idx]\n",
    "                val_x = x_data_training[val_idx]\n",
    "                val_y = y_data_training[val_idx]\n",
    "                run_train(sess, train_x, train_y)\n",
    "                results.append(sess.run(cost, feed_dict={X: val_x, Y: val_y}))\n",
    "            return results\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            result = cross_validate(sess)\n",
    "            result_mean=np.mean(result)\n",
    "            cost_=sess.run(cost, feed_dict={X: x_data_test, Y: y_data_test})\n",
    "            print(\"Cross-validation result: %s\" % result)\n",
    "            print(\"Test cost: %f\" % sess.run(cost, feed_dict={X: x_data_test, Y: y_data_test}))\n",
    "            y_predict_tr = sess.run(hypothesis, feed_dict={X: x_data_training})\n",
    "            y_predict = sess.run(hypothesis, feed_dict={X: x_data_test})\n",
    "            r2_score_tr= explained_variance_score(y_data_training, y_predict_tr)\n",
    "            r2_score= r2_score(y_data_test, y_predict, multioutput='variance_weighted')\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, 'save_path' %(model,data_split_type,condition,name,i))\n",
    "            \n",
    "            group_cost_tr.append(result_mean)\n",
    "            group_cost.append(cost_)\n",
    "            group_r2_score_tr.append(r2_score_tr)\n",
    "            group_r2_score.append(r2_score)\n",
    "            \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,i),y_predict, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,i),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_predict)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,i), dpi=300)\n",
    "    plt.show()        \n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#ANN_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "\n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups)  \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            regr = LinearRegression(n_jobs=-1)\n",
    "            regr.fit(x_data_training_,y_data_training_) \n",
    "\n",
    "            y_pred_tr=regr.predict(x_data_training_)\n",
    "            y_pred=regr.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_=regr.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "        \n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')        \n",
    "\n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups)  \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            regr = Ridge(random_state=0,alpha=0.001) \n",
    "            regr.fit(x_data_training_,y_data_training_)  \n",
    "\n",
    "            y_pred_tr=regr.predict(x_data_training_)\n",
    "            y_pred=regr.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_=regr.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "        \n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#Ridge_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "\n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups) \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            regr = linear_model.Lasso(random_state=0,alpha=0.00001) \n",
    "            regr.fit(x_data_training_,y_data_training_)   \n",
    "\n",
    "            y_pred_tr=regr.predict(x_data_training_)\n",
    "            y_pred=regr.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j)) ##모델 저장\n",
    "        \n",
    "        y_pred_=regr.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#Lasso_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "        \n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups)  \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            regr = linear_model.LassoLars(alpha=0.0001)\n",
    "            regr.fit(x_data_training_,y_data_training_)   \n",
    "            \n",
    "            y_pred_tr=regr.predict(x_data_training_)\n",
    "            y_pred=regr.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_=regr.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#LARS_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "        \n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups) \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            regr = ElasticNet(random_state=0,l1_ratio=0.7, alpha=1e-08)\n",
    "            regr.fit(x_data_training_,y_data_training_) \n",
    "            \n",
    "            y_pred_tr=regr.predict(x_data_training_)\n",
    "            y_pred=regr.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_=regr.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#ENR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "kernel = 1.0 * Matern(nu=0.5,length_scale=1)\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "        \n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups) \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            KRR_regr = KernelRidge(kernel=kernel, alpha=0.1)\n",
    "            KRR_regr.fit(x_data_training_,y_data_training_) \n",
    "            \n",
    "            y_pred_tr=KRR_regr.predict(x_data_training_)\n",
    "            y_pred=KRR_regr.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_=KRR_regr.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "        \n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups)  \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            Bay_Reg =MultiOutputRegressor(linear_model.BayesianRidge(tol=0.1))\n",
    "            Bay_Reg.fit(x_data_training_,y_data_training_) \n",
    "            \n",
    "            y_pred_tr=Bay_Reg.predict(x_data_training_)\n",
    "            y_pred=Bay_Reg.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_=Bay_Reg.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#BRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "        \n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups) \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            regr = MultiOutputRegressor(linear_model.ARDRegression(alpha_1=1e-04,alpha_2=1e-08,lambda_1=1e-08,lambda_2=1e-04))\n",
    "            regr.fit(x_data_training_,y_data_training_) \n",
    "            \n",
    "            y_pred_tr=regr.predict(x_data_training_)\n",
    "            y_pred=regr.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_=regr.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#ARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "        \n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups)  \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            forest_reg = RandomForestRegressor(max_features='sqrt', random_state=0, n_estimators=150, max_depth=15)\n",
    "            forest_reg.fit(x_data_training_,y_data_training_) \n",
    "            \n",
    "            y_pred_tr=forest_reg.predict(x_data_training_)\n",
    "            y_pred=forest_reg.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_=forest_reg.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "        \n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups) \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            Ada_regr = MultiOutputRegressor(AdaBoostRegressor(random_state=0, loss='exponential',\n",
    "                                                          learning_rate=0.1, n_estimators=100))\n",
    "            Ada_regr.fit(x_data_training_,y_data_training_) \n",
    "            \n",
    "            y_pred_tr=Ada_regr.predict(x_data_training_)\n",
    "            y_pred=Ada_regr.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_=Ada_regr.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "        \n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups)\n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            gradientboost_reg = MultiOutputRegressor(GradientBoostingRegressor(loss='ls',learning_rate=0.1, n_estimators=150,subsample=0.7, \n",
    "                                                   random_state=0))\n",
    "            gradientboost_reg.fit(x_data_training_,y_data_training_) \n",
    "            \n",
    "            y_pred_tr=gradientboost_reg.predict(x_data_training_)\n",
    "            y_pred=gradientboost_reg.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_=gradientboost_reg.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "        \n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups) \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            xg_reg =MultiOutputRegressor(xgb.XGBRegressor(learning_rate=0.1,subsample=0.7, max_depth=8, random_state=0))\n",
    "            xg_reg.fit(x_data_training_,y_data_training_) \n",
    "            \n",
    "            y_pred_tr=xg_reg.predict(x_data_training_)\n",
    "            y_pred=xg_reg.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_=xg_reg.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "kernel = 1.0 * Matern(length_scale=1.0, nu=0.5)\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "        \n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups) \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            svr_reg = svr_reg = MultiOutputRegressor(SVR(kernel=kernel, C=1))\n",
    "            svr_reg.fit(x_data_training_,y_data_training_) \n",
    "            \n",
    "            y_pred_tr=svr_reg.predict(x_data_training_)\n",
    "            y_pred=svr_reg.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_=svr_reg.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "        \n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups)  \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            knn_reg = KNeighborsRegressor(weights='distance', n_neighbors= 13, p=1 )\n",
    "            knn_reg.fit(x_data_training_,y_data_training_) \n",
    "            \n",
    "            y_pred_tr=knn_reg.predict(x_data_training_)\n",
    "            y_pred=knn_reg.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_=knn_reg.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='lasso' \n",
    "condition= 'press' #release\n",
    "data_split_type= 'total_split' #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "        \n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -4]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -4]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups)  \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            regr = PLSRegression(n_components=1,tol=1e-08)\n",
    "            regr.fit(x_data_training_,y_data_training_) \n",
    "            \n",
    "            y_pred_tr=regr.predict(x_data_training_)\n",
    "            y_pred=regr.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_=regr.predict(x_data_test)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        \n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test,y_pred_)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pickle\n",
    "from joblib import dump, load \n",
    "\n",
    "kernel = 1.0 * Matern(length_scale=10.0, nu=0.5) \n",
    "start = time.time()\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "sector=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "model='gpr' \n",
    "condition= 'release'    #release , #press\n",
    "data_split_type= 'total_split'  #total_split, #cycle_split\n",
    "group_cost_tr=[]\n",
    "group_cost=[]\n",
    "group_r2_score_tr=[]\n",
    "group_r2_score=[]\n",
    "\n",
    "for j in range(5,6):\n",
    "    col=0\n",
    "    row=2\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize = (8,8))\n",
    "    \n",
    "    for name in sector:\n",
    "        r2_tr=0\n",
    "        r2_val=0\n",
    "        MSE_tr=0\n",
    "        MSE_val=0\n",
    "        train = []\n",
    "        val = []\n",
    "        train_p = []\n",
    "        val_p = []\n",
    "        \n",
    "        if col > 2:\n",
    "            col=0\n",
    "            row-=1\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_tr = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        xy_te = np.loadtxt(\"Input_data_path\" %(data_split_type,name,j), delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        xy_data_tr=xy_tr.reshape(-1,21)\n",
    "        xy_data_te=xy_te.reshape(-1,21)\n",
    "\n",
    "        np.random.shuffle(xy_data_tr)\n",
    "        np.random.shuffle(xy_data_te)\n",
    "        \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_tr , delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), xy_data_te , delimiter=',',fmt='%4f')\n",
    "\n",
    "        x_data_training = xy_data_tr[:, :-5]\n",
    "        y_data_training = xy_data_tr[:, -5]\n",
    "        x_data_test = xy_data_te[:, :-5]\n",
    "        y_data_test = xy_data_te[:, -5]\n",
    "\n",
    "        y_data_training = y_data_training.reshape(-1,1)\n",
    "        y_data_test = y_data_test.reshape(-1,1)\n",
    "        \n",
    "        groups = np.zeros((len(xy_tr)))\n",
    "        cut_data= int(len(xy_tr)/5)\n",
    "        for i in range(int(len(groups)/cut_data)):\n",
    "            groups[i*cut_data:(i*cut_data)+cut_data]=i\n",
    "            \n",
    "        logo = LeaveOneGroupOut()\n",
    "        logo.get_n_splits(x_data_training, y_data_training, groups)\n",
    "\n",
    "        logo.get_n_splits(groups=groups)  \n",
    "\n",
    "        for train_index, val_index in logo.split(x_data_training, y_data_training, groups):\n",
    "\n",
    "            x_data_training_, x_data_val = x_data_training[train_index], x_data_training[val_index]\n",
    "            y_data_training_, y_data_val = y_data_training[train_index], y_data_training[val_index]\n",
    "            \n",
    "            gpr = GaussianProcessRegressor(random_state=0,alpha=1e-04,  kernel= kernel)\n",
    "            gpr.fit(x_data_training_,y_data_training_) \n",
    "            \n",
    "            y_pred_tr=gpr.predict(x_data_training_)\n",
    "            y_pred=gpr.predict(x_data_val)\n",
    "            \n",
    "            r2_tr+=r2_score(y_data_training_, y_pred_tr, multioutput='variance_weighted')\n",
    "            r2_val+=r2_score(y_data_val, y_pred, multioutput='variance_weighted')\n",
    "            MSE_tr+=mean_squared_error(y_data_training_, y_pred_tr, multioutput='uniform_average')\n",
    "            MSE_val+=mean_squared_error(y_data_val, y_pred, multioutput='uniform_average')\n",
    "\n",
    "            train.append(y_data_training_)\n",
    "            val.append(y_data_val)\n",
    "            train_p.append(y_pred_tr)\n",
    "            val_p.append(y_pred)\n",
    "        \n",
    "        dump(regr, 'save_path' %(model,data_split_type,condition,name,j))\n",
    "        \n",
    "        y_pred_, std_te=gpr.predict(x_data_test, return_std=True)\n",
    "        std_te_=std_te.reshape(-1,1)\n",
    "        r2_te_=r2_score(y_data_test, y_pred_, multioutput='variance_weighted')\n",
    "        MSE_te_=mean_squared_error(y_data_test, y_pred_, multioutput='uniform_average')  \n",
    "        y_pred_std_te_p = y_pred_+std_te_\n",
    "        y_pred_std_te_m = y_pred_-std_te_\n",
    "        group_cost_tr.append(MSE_tr/5)\n",
    "        group_cost.append(MSE_te_)\n",
    "        group_r2_score_tr.append(r2_tr/5)\n",
    "        group_r2_score.append(r2_te_)\n",
    "                \n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_std_te_p, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j), y_pred_std_te_m, delimiter=',',fmt='%4f')\n",
    "        np.savetxt(\"save_path\" %(model,data_split_type,condition,name,j),y_data_test, delimiter=',',fmt='%4f')\n",
    "\n",
    "        axes[row,col].plot([0,1])\n",
    "        axes[row,col].scatter(y_data_test, y_pred_)\n",
    "        axes[row,col].scatter(y_data_test, y_pred_std_te_p)\n",
    "        axes[row,col].scatter(y_data_test, y_pred_std_te_m)\n",
    "        col+=1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.savefig('save_path' %(model,data_split_type,condition,j), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "group_cost_tr=np.array(group_cost_tr).reshape(-1,1)\n",
    "group_cost=np.array(group_cost).reshape(-1,1)\n",
    "group_r2_score_tr=np.array(group_r2_score_tr).reshape(-1,1)\n",
    "group_r2_score=np.array(group_r2_score).reshape(-1,1)\n",
    "result_cost=np.concatenate((group_cost_tr,group_cost), axis=1)\n",
    "result_r2_score=np.concatenate((group_r2_score_tr,group_r2_score), axis=1)\n",
    "result_cost_r2_score=np.concatenate((result_cost,result_r2_score), axis=1)\n",
    "\n",
    "np.savetxt(\"save_path\" %(model,data_split_type,condition),result_cost_r2_score, delimiter=',',fmt='%4f')\n",
    "\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "#GPR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
